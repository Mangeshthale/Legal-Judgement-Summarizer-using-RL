{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13681942,"sourceType":"datasetVersion","datasetId":8700718},{"sourceId":646602,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":487647,"modelId":503068}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport kagglehub\nimport os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer, AutoModel\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.translate import meteor_score\n\nnltk.download('punkt', quiet=True)\nnltk.download('wordnet', quiet=True)\n\n# Configure pandas\npd.set_option('display.max_colwidth', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.width', None)\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Check BERTScore\ntry:\n    from bert_score import score as bertscore_fn\n    BERTSCORE_AVAILABLE = True\n    print(\"‚úÖ BERTScore available\")\nexcept ImportError:\n    BERTSCORE_AVAILABLE = False\n    print(\"‚ö†Ô∏è  BERTScore not available\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T16:21:17.838068Z","iopub.execute_input":"2025-11-15T16:21:17.838617Z","iopub.status.idle":"2025-11-15T16:21:17.846142Z","shell.execute_reply.started":"2025-11-15T16:21:17.838594Z","shell.execute_reply":"2025-11-15T16:21:17.845490Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n‚ö†Ô∏è  BERTScore not available\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ============================================================================\n# STEP 1: Copy Essential Classes from Training Script\n# ============================================================================\n\nclass SentenceEncoder(nn.Module):\n    def __init__(self, modelname=\"law-ai/InLegalBERT\", hiddendim=768):\n        super(SentenceEncoder, self).__init__()\n        print(f\"Loading {modelname}...\")\n        self.tokenizer = AutoTokenizer.from_pretrained(modelname)\n        self.model = AutoModel.from_pretrained(modelname)\n        self.hiddendim = hiddendim\n        \n        for param in self.model.parameters():\n            param.requires_grad = False\n        \n        print(f\"{modelname} loaded successfully\")\n    \n    def mean_pooling(self, model_output, attention_mask):\n        token_embeddings = model_output[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    \n    def forward(self, sentences):\n        encoded = self.tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors='pt')\n        encoded = {k: v.to(device) for k, v in encoded.items()}\n        \n        with torch.no_grad():\n            model_output = self.model(**encoded)\n        \n        embeddings = self.mean_pooling(model_output, encoded['attention_mask'])\n        return embeddings\n\n\nclass MultiAspectPolicyNetwork(nn.Module):\n    \"\"\"CORRECTED: Uses underscores to match saved model\"\"\"\n    def __init__(self, input_dim=768, hidden_dim=256, num_aspects=5, dropout=0.5):\n        super(MultiAspectPolicyNetwork, self).__init__()\n        \n        self.num_aspects = num_aspects\n        self.aspects = ['facts', 'analysis', 'argument', 'judgement', 'statute']\n        self.hidden_dim = hidden_dim\n        \n        # NOTE: Use UNDERSCORES (not camelCase) to match saved model\n        self.shared_lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2, \n                                   bidirectional=True, batch_first=True, dropout=dropout)\n        \n        self.position_embedding = nn.Embedding(2000, 64)\n        self.aspect_embedding = nn.Embedding(num_aspects, hidden_dim * 2)\n        \n        self.attention = nn.MultiheadAttention(\n            embed_dim=hidden_dim * 2,\n            num_heads=4, \n            dropout=dropout,\n            batch_first=True\n        )\n        \n        # Use underscores in dict key\n        self.aspect_heads = nn.ModuleDict()\n        for aspect in self.aspects:\n            self.aspect_heads[aspect] = nn.Sequential(\n                nn.Linear(hidden_dim * 2 + 64 + hidden_dim * 2, 512),\n                nn.LayerNorm(512),\n                nn.ReLU(),\n                nn.Dropout(dropout),\n                nn.Linear(512, 256),\n                nn.LayerNorm(256),\n                nn.ReLU(),\n                nn.Dropout(dropout),\n                nn.Linear(256, 1)\n            )\n    \n    def forward(self, sentence_embeddings, positions, aspect_idx):\n        lstm_out, _ = self.shared_lstm(sentence_embeddings)\n        \n        aspect_emb_query = self.aspect_embedding(torch.tensor([aspect_idx], device=device))\n        aspect_emb_query = aspect_emb_query.unsqueeze(1).expand(-1, lstm_out.size(1), -1)\n        \n        attended_out, _ = self.attention(aspect_emb_query, lstm_out, lstm_out)\n        \n        combined_lstm = lstm_out + attended_out\n        pos_emb = self.position_embedding(positions)\n        aspect_emb_concat = self.aspect_embedding(torch.tensor([aspect_idx], device=device))\n        aspect_emb_concat = aspect_emb_concat.unsqueeze(1).expand(-1, sentence_embeddings.size(1), -1)\n        \n        combined = torch.cat([combined_lstm, pos_emb, aspect_emb_concat], dim=-1)\n        \n        aspect_name = self.aspects[aspect_idx]\n        logits = self.aspect_heads[aspect_name](combined).squeeze(-1)\n        \n        return logits\n\nprint(\"‚úÖ Policy Network defined with CORRECT naming (underscores)\")\nclass UnsupervisedRLAgent:\n    def __init__(self, encoder, policy):\n        self.encoder = encoder.to(device)\n        self.policy = policy.to(device)\n        self.aspects = ['facts', 'analysis', 'argument', 'judgement', 'statute']\n        self.aspectsummaryratios = {\n            'facts': 0.12,\n            'analysis': 0.12,\n            'argument': 0.08,\n            'judgement': 0.06,\n            'statute': 0.08\n        }\n        self.minsummarysentences = 3\n        self.maxdocumentsentences = 500\n    \n    def preprocess_document(self, judgmenttext):\n        sentences = sent_tokenize(judgmenttext)\n        sentences = [s.strip() for s in sentences if len(s.strip().split()) > 5]\n        \n        if len(sentences) > self.maxdocumentsentences:\n            print(f\"Truncating {len(sentences)} to {self.maxdocumentsentences} sentences\")\n            sentences = sentences[:self.maxdocumentsentences]\n        \n        return sentences\n    \n    def encode_sentences(self, sentences):\n        if len(sentences) == 0:\n            return torch.zeros(1, self.encoder.hiddendim).to(device)\n        \n        batchsize = 16\n        embeddings = []\n        \n        for i in range(0, len(sentences), batchsize):\n            batch = sentences[i:i+batchsize]\n            emb = self.encoder(batch)\n            embeddings.append(emb)\n        \n        return torch.cat(embeddings, dim=0)\n    \n    def generate_summaries(self, judgment):\n        self.policy.eval()\n        \n        with torch.no_grad():\n            sentences = self.preprocess_document(judgment)\n            \n            if len(sentences) < 3:\n                return {aspect: \" \".join(sentences) for aspect in self.aspects}\n            \n            sentenceembeddings = self.encode_sentences(sentences)\n            sentenceembeddings = sentenceembeddings.unsqueeze(0)\n            \n            positions = torch.arange(min(len(sentences), 1999), device=device).unsqueeze(0)\n            if len(sentences) < positions.size(1):\n                positions = positions[:, :len(sentences)]\n            \n            summaries = {}\n            \n            for aspectidx, aspect in enumerate(self.aspects):\n                logits = self.policy(sentenceembeddings, positions, aspectidx).squeeze(0)\n                aspectratio = self.aspectsummaryratios[aspect]\n                numselect = max(self.minsummarysentences, int(len(sentences) * aspectratio))\n                topkindices = torch.topk(logits, k=numselect).indices\n                topkindices = sorted(topkindices.cpu().numpy())\n                summary = \" \".join([sentences[i] for i in topkindices])\n                summaries[aspect] = summary\n            \n            return summaries\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T16:21:17.848219Z","iopub.execute_input":"2025-11-15T16:21:17.848657Z","iopub.status.idle":"2025-11-15T16:21:17.869419Z","shell.execute_reply.started":"2025-11-15T16:21:17.848640Z","shell.execute_reply":"2025-11-15T16:21:17.868672Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Policy Network defined with CORRECT naming (underscores)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ============================================================================\n# STEP 2: Metrics Evaluator Class (NEW - For Reference-Based Evaluation)\n# ============================================================================\n\nclass MetricsEvaluator:\n    \"\"\"Compute ROUGE, BLEU, METEOR, and BERTScore metrics\"\"\"\n    \n    def __init__(self):\n        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n        self.smoothing = SmoothingFunction().method1\n    \n    def compute_all_metrics(self, generated_summary, reference_summary, source_document):\n        \"\"\"Compute all metrics for a generated summary against reference\"\"\"\n        \n        metrics = {}\n        \n        # ROUGE scores\n        rouge_scores = self.rouge_scorer.score(reference_summary, generated_summary)\n        metrics['rouge1_precision'] = rouge_scores['rouge1'].precision\n        metrics['rouge1_recall'] = rouge_scores['rouge1'].recall\n        metrics['rouge1_fmeasure'] = rouge_scores['rouge1'].fmeasure\n        metrics['rouge2_precision'] = rouge_scores['rouge2'].precision\n        metrics['rouge2_recall'] = rouge_scores['rouge2'].recall\n        metrics['rouge2_fmeasure'] = rouge_scores['rouge2'].fmeasure\n        metrics['rougeL_precision'] = rouge_scores['rougeL'].precision\n        metrics['rougeL_recall'] = rouge_scores['rougeL'].recall\n        metrics['rougeL_fmeasure'] = rouge_scores['rougeL'].fmeasure\n        \n        # BLEU scores\n        reference_tokens = reference_summary.split()\n        generated_tokens = generated_summary.split()\n        \n        try:\n            metrics['bleu1'] = sentence_bleu([reference_tokens], generated_tokens, weights=(1, 0, 0, 0), smoothing_function=self.smoothing)\n            metrics['bleu2'] = sentence_bleu([reference_tokens], generated_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=self.smoothing)\n            metrics['bleu4'] = sentence_bleu([reference_tokens], generated_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=self.smoothing)\n        except:\n            metrics['bleu1'] = 0.0\n            metrics['bleu2'] = 0.0\n            metrics['bleu4'] = 0.0\n        \n        # METEOR score\n        try:\n            metrics['meteor'] = meteor_score.meteor_score([reference_tokens], generated_tokens)\n        except:\n            metrics['meteor'] = 0.0\n        \n        # BERTScore\n        if BERTSCORE_AVAILABLE:\n            try:\n                P, R, F1 = bertscore_fn([generated_summary], [reference_summary], lang='en', rescale_with_baseline=True)\n                metrics['bertscore_precision'] = P.item()\n                metrics['bertscore_recall'] = R.item()\n                metrics['bertscore_f1'] = F1.item()\n            except:\n                metrics['bertscore_precision'] = 0.0\n                metrics['bertscore_recall'] = 0.0\n                metrics['bertscore_f1'] = 0.0\n        \n        # Length statistics\n        metrics['generated_length'] = len(generated_tokens)\n        metrics['reference_length'] = len(reference_tokens)\n        metrics['source_length'] = len(source_document.split())\n        metrics['length_ratio'] = metrics['generated_length'] / max(metrics['reference_length'], 1)\n        metrics['compression_ratio'] = metrics['generated_length'] / max(metrics['source_length'], 1)\n        \n        return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T16:21:17.994596Z","iopub.execute_input":"2025-11-15T16:21:17.994957Z","iopub.status.idle":"2025-11-15T16:21:18.004232Z","shell.execute_reply.started":"2025-11-15T16:21:17.994941Z","shell.execute_reply":"2025-11-15T16:21:18.003408Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T16:21:18.005471Z","iopub.execute_input":"2025-11-15T16:21:18.005694Z","iopub.status.idle":"2025-11-15T16:21:18.031918Z","shell.execute_reply.started":"2025-11-15T16:21:18.005679Z","shell.execute_reply":"2025-11-15T16:21:18.030896Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nüìä LOADING VALIDATION DATASET\n======================================================================\n\nJudgment directory: /kaggle/input/summaries/IN-Ext/judgement\nSummary base directory: /kaggle/input/summaries/IN-Ext/summary/segment-wise/A2\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2507685053.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Summary directory not found: {summary_base_dir}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mvalidation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiAspectLegalDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudgment_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_base_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'MultiAspectLegalDataset' is not defined"],"ename":"NameError","evalue":"name 'MultiAspectLegalDataset' is not defined","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: Dataset Class\n# ============================================================================\n\nclass MultiAspectLegalDataset(Dataset):\n    def __init__(self, judgment_dir, summary_base_dir):\n        self.aspects = ['facts', 'analysis', 'argument', 'judgement', 'statute']\n        self.judgment_files = sorted(glob.glob(os.path.join(judgment_dir, '*.txt')))\n        \n        print(f\"üìÅ Found {len(self.judgment_files)} judgments\")\n        \n        self.aspect_summary_files = {}\n        for aspect in self.aspects:\n            aspect_path = os.path.join(summary_base_dir, aspect)\n            files = sorted(glob.glob(os.path.join(aspect_path, '*.txt')))\n            self.aspect_summary_files[aspect] = files\n            print(f\"   {aspect:12s}: {len(files)} summaries\")\n        \n        self.data = []\n        for idx, jf in enumerate(self.judgment_files):\n            with open(jf, 'r', encoding='utf-8', errors='ignore') as f:\n                judgment = f.read().strip()\n            \n            if not judgment:\n                continue\n            \n            aspect_summaries = {}\n            has_valid_summary = False\n            \n            for aspect in self.aspects:\n                if idx < len(self.aspect_summary_files[aspect]):\n                    summary_file = self.aspect_summary_files[aspect][idx]\n                    with open(summary_file, 'r', encoding='utf-8', errors='ignore') as f:\n                        summary = f.read().strip()\n                    \n                    if summary and len(summary.split()) > 10:\n                        aspect_summaries[aspect] = summary\n                        has_valid_summary = True\n                    else:\n                        aspect_summaries[aspect] = None\n                else:\n                    aspect_summaries[aspect] = None\n            \n            if has_valid_summary:\n                self.data.append({\n                    'judgment': judgment,\n                    'summaries': aspect_summaries,\n                    'judgment_file': os.path.basename(jf)\n                })\n        \n        print(f\"\\nüìä Total valid samples loaded: {len(self.data)}\")\n        for aspect in self.aspects:\n            valid_count = sum(1 for item in self.data if item['summaries'][aspect] is not None)\n            print(f\"   {aspect:12s}: {valid_count} valid summaries\")\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T16:21:45.695823Z","iopub.execute_input":"2025-11-15T16:21:45.696521Z","iopub.status.idle":"2025-11-15T16:21:45.705316Z","shell.execute_reply.started":"2025-11-15T16:21:45.696496Z","shell.execute_reply":"2025-11-15T16:21:45.704495Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# ============================================================================\n# STEP 3: Load Validation Dataset with Correct Paths\n# ============================================================================\npath  = \"/kaggle/input/summaries\"\nprint(\"\\n\" + \"=\"*70)\nprint(\"üìä LOADING VALIDATION DATASET\")\nprint(\"=\"*70 + \"\\n\")\n\n# Set correct paths based on the dataset structure\njudgment_dir = os.path.join(path,  'IN-Ext', 'judgement')\nsummary_base_dir = os.path.join(path,  'IN-Ext', 'summary', 'segment-wise', 'A2')\n\nprint(f\"Judgment directory: {judgment_dir}\")\nprint(f\"Summary base directory: {summary_base_dir}\\n\")\n\n# Verify paths exist\nif not os.path.exists(judgment_dir):\n    raise FileNotFoundError(f\"Judgment directory not found: {judgment_dir}\")\nif not os.path.exists(summary_base_dir):\n    raise FileNotFoundError(f\"Summary directory not found: {summary_base_dir}\")\n\nvalidation_dataset = MultiAspectLegalDataset(judgment_dir, summary_base_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T16:22:41.548935Z","iopub.execute_input":"2025-11-15T16:22:41.549268Z","iopub.status.idle":"2025-11-15T16:22:43.378036Z","shell.execute_reply.started":"2025-11-15T16:22:41.549245Z","shell.execute_reply":"2025-11-15T16:22:43.377283Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nüìä LOADING VALIDATION DATASET\n======================================================================\n\nJudgment directory: /kaggle/input/summaries/IN-Ext/judgement\nSummary base directory: /kaggle/input/summaries/IN-Ext/summary/segment-wise/A2\n\nüìÅ Found 50 judgments\n   facts       : 50 summaries\n   analysis    : 50 summaries\n   argument    : 46 summaries\n   judgement   : 50 summaries\n   statute     : 41 summaries\n\nüìä Total valid samples loaded: 50\n   facts       : 50 valid summaries\n   analysis    : 50 valid summaries\n   argument    : 46 valid summaries\n   judgement   : 47 valid summaries\n   statute     : 37 valid summaries\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# ============================================================================\n# STEP 5: Load Trained Model\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üîÑ LOADING TRAINED MODEL\")\nprint(\"=\"*70 + \"\\n\")\n\n# Initialize encoder and policy\nencoder = SentenceEncoder(modelname=\"law-ai/InLegalBERT\", hiddendim=768)\npolicy = MultiAspectPolicyNetwork(input_dim=768, hidden_dim=256, num_aspects=5, dropout=0.5)\n\n# Load checkpoint\nmodel_path = \"/kaggle/input/inlegal-50-data-unsupervised/pytorch/default/1/inlegalbert-50-unsupervised_legal_summarization.pt\"\ncheckpoint = torch.load(model_path, map_location=device, weights_only=False)\n\n# Get state dicts\nstate_dict = checkpoint['policy_state_dict']\ncurrent_model_dict = policy.state_dict()\n\n# Handle position embedding size mismatch\nif 'position_embedding.weight' in state_dict:\n    old_pos_emb = state_dict['position_embedding.weight']\n    new_pos_emb = current_model_dict['position_embedding.weight']\n    \n    if old_pos_emb.shape[0] != new_pos_emb.shape[0]:\n        print(f\"‚ö†Ô∏è  Resizing position embeddings: {old_pos_emb.shape[0]} ‚Üí {new_pos_emb.shape[0]}\")\n        \n        old_size, emb_dim = old_pos_emb.shape\n        new_size = new_pos_emb.shape[0]\n        \n        # Create resized embedding\n        resized_pos_emb = torch.zeros(new_size, emb_dim)\n        resized_pos_emb[:old_size, :] = old_pos_emb  # Copy trained weights\n        \n        # Initialize new positions (501-2000) with small random values\n        if new_size > old_size:\n            resized_pos_emb[old_size:, :] = torch.randn(new_size - old_size, emb_dim) * 0.02\n        \n        state_dict['position_embedding.weight'] = resized_pos_emb\n\n# Load the corrected state dict\npolicy.load_state_dict(state_dict)\n\n# Create agent\nagent = UnsupervisedRLAgent(encoder=encoder, policy=policy)\nagent.policy.eval()\nagent.policy.to(device)\n\nprint(f\"‚úÖ Model loaded successfully from {model_path}\")\nprint(f\"üìä Training completed at epoch {checkpoint['epoch']}\")\nprint(\"=\"*70 + \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T16:27:52.253757Z","iopub.execute_input":"2025-11-15T16:27:52.254141Z","iopub.status.idle":"2025-11-15T16:27:53.719566Z","shell.execute_reply.started":"2025-11-15T16:27:52.254113Z","shell.execute_reply":"2025-11-15T16:27:53.718916Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nüîÑ LOADING TRAINED MODEL\n======================================================================\n\nLoading law-ai/InLegalBERT...\nlaw-ai/InLegalBERT loaded successfully\n‚ö†Ô∏è  Resizing position embeddings: 500 ‚Üí 2000\n‚úÖ Model loaded successfully from /kaggle/input/inlegal-50-data-unsupervised/pytorch/default/1/inlegalbert-50-unsupervised_legal_summarization.pt\nüìä Training completed at epoch 1\n======================================================================\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ============================================================================\n# STEP 6: Evaluate with Reference Summaries\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üìä COMPREHENSIVE VALIDATION SET EVALUATION WITH REFERENCE SUMMARIES\")\nprint(\"=\"*70 + \"\\n\")\n\n# Initialize metrics evaluator\nmetrics_evaluator = MetricsEvaluator()\n\n# Collect metrics for each aspect\naspect_metrics = {aspect: [] for aspect in agent.aspects}\n\nprint(\"Computing all metrics on validation set...\\n\")\n\nfor idx, item in enumerate(validation_dataset):\n    judgment = item['judgment']\n    reference_summaries = item['summaries']\n    \n    # Generate summaries\n    generated_summaries = agent.generate_summaries(judgment)\n    \n    # Compute metrics for each aspect\n    for aspect in agent.aspects:\n        if reference_summaries[aspect]:\n            metrics = metrics_evaluator.compute_all_metrics(\n                generated_summaries[aspect],\n                reference_summaries[aspect],\n                judgment\n            )\n            aspect_metrics[aspect].append(metrics)\n    \n    if (idx + 1) % 10 == 0:\n        print(f\"Processed {idx + 1}/{len(validation_dataset)} samples...\")\n\nprint(f\"\\n{'='*70}\")\nprint(\"VALIDATION RESULTS\")\nprint(f\"{'='*70}\\n\")\n\n# Calculate average metrics for each aspect\nfor aspect in agent.aspects:\n    if aspect_metrics[aspect]:\n        print(f\"\\n{'='*70}\")\n        print(f\"üìà {aspect.upper()} - DETAILED METRICS\")\n        print(f\"{'='*70}\")\n        \n        # Average all metrics\n        avg_metrics = {}\n        for key in aspect_metrics[aspect][0].keys():\n            avg_metrics[key] = np.mean([m[key] for m in aspect_metrics[aspect]])\n        \n        print(f\"\\nüî¥ ROUGE Scores:\")\n        print(f\"   ROUGE-1:  P={avg_metrics['rouge1_precision']:.4f}  R={avg_metrics['rouge1_recall']:.4f}  F1={avg_metrics['rouge1_fmeasure']:.4f}\")\n        print(f\"   ROUGE-2:  P={avg_metrics['rouge2_precision']:.4f}  R={avg_metrics['rouge2_recall']:.4f}  F1={avg_metrics['rouge2_fmeasure']:.4f}\")\n        print(f\"   ROUGE-L:  P={avg_metrics['rougeL_precision']:.4f}  R={avg_metrics['rougeL_recall']:.4f}  F1={avg_metrics['rougeL_fmeasure']:.4f}\")\n        \n        print(f\"\\nüîµ BLEU Scores:\")\n        print(f\"   BLEU-1:   {avg_metrics['bleu1']:.4f}\")\n        print(f\"   BLEU-2:   {avg_metrics['bleu2']:.4f}\")\n        print(f\"   BLEU-4:   {avg_metrics['bleu4']:.4f}\")\n        \n        print(f\"\\nüü¢ Other Metrics:\")\n        print(f\"   METEOR:   {avg_metrics['meteor']:.4f}\")\n        \n        if BERTSCORE_AVAILABLE:\n            print(f\"\\nüü° BERTScore:\")\n            print(f\"   Precision: {avg_metrics['bertscore_precision']:.4f}\")\n            print(f\"   Recall:    {avg_metrics['bertscore_recall']:.4f}\")\n            print(f\"   F1:        {avg_metrics['bertscore_f1']:.4f}\")\n        \n        print(f\"\\nüìè Length Statistics:\")\n        print(f\"   Generated Length:    {avg_metrics['generated_length']:.0f} words\")\n        print(f\"   Reference Length:    {avg_metrics['reference_length']:.0f} words\")\n        print(f\"   Source Length:       {avg_metrics['source_length']:.0f} words\")\n        print(f\"   Length Ratio:        {avg_metrics['length_ratio']:.2f}\")\n        print(f\"   Compression Ratio:   {avg_metrics['compression_ratio']:.2%}\")\n    else:\n        print(f\"\\n{aspect}: No validation samples available\")\n\nprint(f\"\\n{'='*70}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T16:28:07.564828Z","iopub.execute_input":"2025-11-15T16:28:07.565618Z","iopub.status.idle":"2025-11-15T16:29:24.555203Z","shell.execute_reply.started":"2025-11-15T16:28:07.565591Z","shell.execute_reply":"2025-11-15T16:29:24.554480Z"}},"outputs":[{"name":"stdout","text":"\n======================================================================\nüìä COMPREHENSIVE VALIDATION SET EVALUATION WITH REFERENCE SUMMARIES\n======================================================================\n\nComputing all metrics on validation set...\n\nProcessed 10/50 samples...\nProcessed 20/50 samples...\nProcessed 30/50 samples...\nProcessed 40/50 samples...\nProcessed 50/50 samples...\n\n======================================================================\nVALIDATION RESULTS\n======================================================================\n\n\n======================================================================\nüìà FACTS - DETAILED METRICS\n======================================================================\n\nüî¥ ROUGE Scores:\n   ROUGE-1:  P=0.4119  R=0.5836  F1=0.4484\n   ROUGE-2:  P=0.1889  R=0.2775  F1=0.2069\n   ROUGE-L:  P=0.2063  R=0.3097  F1=0.2277\n\nüîµ BLEU Scores:\n   BLEU-1:   0.3550\n   BLEU-2:   0.2377\n   BLEU-4:   0.1433\n\nüü¢ Other Metrics:\n   METEOR:   0.3179\n\nüìè Length Statistics:\n   Generated Length:    599 words\n   Reference Length:    423 words\n   Source Length:       5387 words\n   Length Ratio:        1.94\n   Compression Ratio:   11.07%\n\n======================================================================\nüìà ANALYSIS - DETAILED METRICS\n======================================================================\n\nüî¥ ROUGE Scores:\n   ROUGE-1:  P=0.6575  R=0.5081  F1=0.5554\n   ROUGE-2:  P=0.3377  R=0.2589  F1=0.2841\n   ROUGE-L:  P=0.3354  R=0.2573  F1=0.2819\n\nüîµ BLEU Scores:\n   BLEU-1:   0.4167\n   BLEU-2:   0.2961\n   BLEU-4:   0.1897\n\nüü¢ Other Metrics:\n   METEOR:   0.3217\n\nüìè Length Statistics:\n   Generated Length:    619 words\n   Reference Length:    828 words\n   Source Length:       5387 words\n   Length Ratio:        0.81\n   Compression Ratio:   11.39%\n\n======================================================================\nüìà ARGUMENT - DETAILED METRICS\n======================================================================\n\nüî¥ ROUGE Scores:\n   ROUGE-1:  P=0.2162  R=0.5114  F1=0.2775\n   ROUGE-2:  P=0.0551  R=0.1435  F1=0.0726\n   ROUGE-L:  P=0.1169  R=0.2936  F1=0.1522\n\nüîµ BLEU Scores:\n   BLEU-1:   0.1975\n   BLEU-2:   0.0961\n   BLEU-4:   0.0216\n\nüü¢ Other Metrics:\n   METEOR:   0.2360\n\nüìè Length Statistics:\n   Generated Length:    446 words\n   Reference Length:    176 words\n   Source Length:       5521 words\n   Length Ratio:        3.56\n   Compression Ratio:   8.01%\n\n======================================================================\nüìà JUDGEMENT - DETAILED METRICS\n======================================================================\n\nüî¥ ROUGE Scores:\n   ROUGE-1:  P=0.1920  R=0.6825  F1=0.2656\n   ROUGE-2:  P=0.1059  R=0.3965  F1=0.1504\n   ROUGE-L:  P=0.1386  R=0.5243  F1=0.1957\n\nüîµ BLEU Scores:\n   BLEU-1:   0.1759\n   BLEU-2:   0.1289\n   BLEU-4:   0.0915\n\nüü¢ Other Metrics:\n   METEOR:   0.3749\n\nüìè Length Statistics:\n   Generated Length:    277 words\n   Reference Length:    72 words\n   Source Length:       5186 words\n   Length Ratio:        6.44\n   Compression Ratio:   5.33%\n\n======================================================================\nüìà STATUTE - DETAILED METRICS\n======================================================================\n\nüî¥ ROUGE Scores:\n   ROUGE-1:  P=0.2136  R=0.4065  F1=0.2370\n   ROUGE-2:  P=0.0383  R=0.0712  F1=0.0411\n   ROUGE-L:  P=0.1158  R=0.2369  F1=0.1308\n\nüîµ BLEU Scores:\n   BLEU-1:   0.1616\n   BLEU-2:   0.0655\n   BLEU-4:   0.0062\n\nüü¢ Other Metrics:\n   METEOR:   0.1875\n\nüìè Length Statistics:\n   Generated Length:    437 words\n   Reference Length:    235 words\n   Source Length:       5508 words\n   Length Ratio:        3.70\n   Compression Ratio:   7.77%\n\n======================================================================\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ============================================================================\n# STEP 7: Display Sample Summaries\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üìÑ SAMPLE SUMMARIES WITH METRICS\")\nprint(\"=\"*70 + \"\\n\")\n\nnum_samples = min(3, len(validation_dataset))\n\nfor idx in range(num_samples):\n    item = validation_dataset[idx]\n    judgment = item['judgment']\n    reference_summaries = item['summaries']\n    judgment_file = item['judgment_file']\n    \n    print(\"\\n\" + \"=\"*70)\n    print(f\"SAMPLE {idx + 1} - File: {judgment_file}\")\n    print(\"=\"*70)\n    \n    sentences = agent.preprocess_document(judgment)\n    source_words = sum(len(s.split()) for s in sentences)\n    print(f\"\\nüìÑ SOURCE DOCUMENT:\")\n    print(f\"   Total Sentences: {len(sentences)}\")\n    print(f\"   Total Words:     {source_words}\")\n    print(f\"\\n   First 300 characters:\")\n    print(f\"   {judgment[:300]}...\\n\")\n    \n    # Generate summaries\n    generated_summaries = agent.generate_summaries(judgment)\n    \n    # Display each aspect\n    for aspect in agent.aspects:\n        print(f\"\\n{'‚îÄ'*70}\")\n        print(f\"üéØ ASPECT: {aspect.upper()}\")\n        print(f\"{'‚îÄ'*70}\")\n        \n        if reference_summaries[aspect]:\n            ref_summary = reference_summaries[aspect]\n            gen_summary = generated_summaries[aspect]\n            \n            # Compute metrics\n            metrics = metrics_evaluator.compute_all_metrics(gen_summary, ref_summary, judgment)\n            \n            print(f\"\\nüìä METRICS:\")\n            print(f\"   ROUGE-1 F1: {metrics['rouge1_fmeasure']:.4f}\")\n            print(f\"   ROUGE-2 F1: {metrics['rouge2_fmeasure']:.4f}\")\n            print(f\"   ROUGE-L F1: {metrics['rougeL_fmeasure']:.4f}\")\n            print(f\"   BLEU-4:     {metrics['bleu4']:.4f}\")\n            print(f\"   METEOR:     {metrics['meteor']:.4f}\")\n            if BERTSCORE_AVAILABLE:\n                print(f\"   BERTScore:  {metrics['bertscore_f1']:.4f}\")\n            print(f\"   Gen Length: {metrics['generated_length']} words\")\n            print(f\"   Ref Length: {metrics['reference_length']} words\")\n            print(f\"   Compression: {metrics['compression_ratio']:.2%}\")\n            \n            print(f\"\\nüìù REFERENCE SUMMARY ({len(ref_summary.split())} words):\")\n            print(f\"{'‚îÄ'*70}\")\n            print(ref_summary + \"...\")\n            \n            print(f\"\\nü§ñ GENERATED SUMMARY ({len(gen_summary.split())} words):\")\n            print(f\"{'‚îÄ'*70}\")\n            print(gen_summary+ \"...\")\n        else:\n            print(f\"\\n   ‚ö†Ô∏è  No reference summary available\")\n    \n    print(\"\\n\" + \"=\"*70 + \"\\n\")\n\nprint(\"\\n‚úÖ Evaluation complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T16:21:18.037709Z","iopub.status.idle":"2025-11-15T16:21:18.038123Z","shell.execute_reply.started":"2025-11-15T16:21:18.037983Z","shell.execute_reply":"2025-11-15T16:21:18.037998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}